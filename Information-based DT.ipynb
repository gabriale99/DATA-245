{"cells":[{"cell_type":"markdown","metadata":{"id":"CjfM30cA-OXw"},"source":["**5. (15%) Chapter 4, exercise 7**\n","\n","**The following table lists a dataset collected in an electronics shop showing details of customers and whether they responded to a special offer to buy a new laptop.**\n","\n","**This dataset has been used to build a decision tree to predict which customers will respond to future special offers. The decision tree, created using the ID3 algorithm, is shown below.**\n","\n","**a. The information gain (calculated using entropy) of the feature AGE at the root node of the tree is 0.247. A colleague has suggested that the STUDENT feature would be better at the root node of the tree. Show that this is not the case.**\n","\n","<pre>\n","                          no                       yes\n","Entropy      = -((5 / 14) * log<sub>2</sub>(5 / 14) + (9 / 14) * log<sub>2</sub>(9 / 14)) = 0.940 (3 sig. fig.)\n","                         no\n","Rem(Student) = ((7 / 14) * (-((4 / 7) * log<sub>2</sub>(4 / 7) + (3 / 7) * log<sub>2</sub>(3 / 7)))) +  <- no\n","               ((7 / 14) * (-((1 / 7) * log<sub>2</sub>(1 / 7) + (6 / 7) * log<sub>2</sub>(6 / 7))))    <- yes\n","             = 0.788 (3 sig. fig.)\n","IG(Student) = 1 - 0.788 = 0.212\n","</pre>\n","\n","Since the information gain from [Student] is less then [Age], [Student] should not be the root of the tree.\n","\n","---\n","\n","**b. Yet another colleague has suggested that the ID feature would be a very effective at the root node of the tree. Would you agree with this suggestion?**\n","\n","No, the ID feature is a unique identifier for each instance of the dataset. It only describes each instance but it doesn't contribute anything to the result of the prediction."]},{"cell_type":"code","execution_count":356,"metadata":{"executionInfo":{"elapsed":252,"status":"ok","timestamp":1677905231973,"user":{"displayName":"hon lam chung","userId":"11702692533387331032"},"user_tz":480},"id":"EJ05OQCw7Atb"},"outputs":[],"source":["# 6. (25 %, coding assignment) Still use sklearn.datasets import load_iris dataset. Can you show how the tree classification performances are affected by various ensemble techniques? \n","import sklearn\n","assert sklearn.__version__ >= \"0.20\"\n","# Scikit-Learn â‰¥0.20 is required\n","\n","from sklearn.datasets import load_iris\n","from sklearn.metrics import accuracy_score\n","from sklearn.model_selection import train_test_split\n","\n","iris = load_iris()\n","X = iris.data\n","y = iris.target\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)"]},{"cell_type":"markdown","metadata":{"id":"SFtU6V3JDTEv"},"source":["The following code snippet shows the accuracy of decision tree model alone."]},{"cell_type":"code","execution_count":366,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1677905353573,"user":{"displayName":"hon lam chung","userId":"11702692533387331032"},"user_tz":480},"id":"FhvRqddq-RxY","outputId":"cac5047a-0d1b-401b-c15f-170a8e573540"},"outputs":[{"name":"stdout","output_type":"stream","text":["0.9333333333333333\n"]}],"source":["from sklearn.tree import DecisionTreeClassifier\n","\n","dt_clf = DecisionTreeClassifier(random_state=42)\n","dt_clf.fit(X_train, y_train)\n","y_pred = dt_clf.predict(X_test)\n","print(accuracy_score(y_test, y_pred))"]},{"cell_type":"markdown","metadata":{"id":"j-rynZgyDbbB"},"source":["The code below shows the ensemble technique, bagging. From the result, we can see that in the majority of the iterations, the technique improves the performance."]},{"cell_type":"code","execution_count":358,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3049,"status":"ok","timestamp":1677905235016,"user":{"displayName":"hon lam chung","userId":"11702692533387331032"},"user_tz":480},"id":"q-usKl-gTSLL","outputId":"18eb93a4-b17b-4ccd-a7b1-782d4431e756"},"outputs":[{"name":"stdout","output_type":"stream","text":["{0.9666666666666667: 55, 0.9333333333333333: 34, 0.9: 11}\n"]}],"source":["from sklearn.ensemble import BaggingClassifier\n","\n","# Bagging\n","def bagging_demo(x_tn, x_tst, y_tn, y_tst):\n","  bag_clf = BaggingClassifier(\n","      DecisionTreeClassifier(random_state=42))\n","  bag_clf.fit(x_tn, y_tn)\n","  y_pred = bag_clf.predict(x_tst)\n","  return accuracy_score(y_tst, y_pred)\n","\n","occurence = {}\n","for i in range(100):\n","  res = bagging_demo(X_train, X_test, y_train, y_test)\n","  if res in occurence:\n","    occurence[res] += 1\n","  else:\n","    occurence[res] = 1\n","\n","print(occurence)"]},{"cell_type":"markdown","metadata":{"id":"41jtd8jhG3WI"},"source":["Random Forest is a combination of bagging, decision tree and subspace sampling. We would expect it returns a similar result as the bagging technique. However, the accuracy of tree classification didn't improve."]},{"cell_type":"code","execution_count":361,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25120,"status":"ok","timestamp":1677905326798,"user":{"displayName":"hon lam chung","userId":"11702692533387331032"},"user_tz":480},"id":"wP_ZqDrTTSNb","outputId":"70f43f9e-5346-4533-f39e-3fa305e14cb4"},"outputs":[{"name":"stdout","output_type":"stream","text":["{0.9333333333333333: 60, 0.9: 16, 0.9666666666666667: 24}\n"]}],"source":["from sklearn.ensemble import RandomForestClassifier\n","\n","# Random Forest\n","def random_forest_demo(x_tn, x_tst, y_tn, y_tst):\n","  rnd_clf = RandomForestClassifier()\n","  rnd_clf.fit(X_train, y_train)\n","  y_pred_rf = rnd_clf.predict(X_test)\n","  return accuracy_score(y_tst, y_pred_rf)\n","\n","occurence = {}\n","for i in range(100):\n","  res = random_forest_demo(X_train, X_test, y_train, y_test)\n","  if res in occurence:\n","    occurence[res] += 1\n","  else:\n","    occurence[res] = 1\n","\n","print(occurence)"]},{"cell_type":"markdown","metadata":{"id":"Fwk2MKL_IXvU"},"source":["Finally, we perform boosting technique using ada boosting. In most iterations, it returns the same accuracy as the decision tree."]},{"cell_type":"code","execution_count":362,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":395,"status":"ok","timestamp":1677905327162,"user":{"displayName":"hon lam chung","userId":"11702692533387331032"},"user_tz":480},"id":"SKuK4NFeTSP4","outputId":"20622ada-0194-4318-b53a-b1d6f02778b7"},"outputs":[{"name":"stdout","output_type":"stream","text":["{0.9333333333333333: 68, 0.9666666666666667: 32}\n"]}],"source":["from sklearn.ensemble import AdaBoostClassifier\n","\n","# Ada Boost\n","def ada_boost_demo(x_tn, x_tst, y_tn, y_tst):\n","  ada_clf = AdaBoostClassifier(\n","      DecisionTreeClassifier(random_state=42))\n","  ada_clf.fit(X_train, y_train)\n","  y_pred_ab = ada_clf.predict(X_test)\n","  return accuracy_score(y_tst, y_pred_ab)\n","\n","occurence = {}\n","for i in range(100):\n","  res = ada_boost_demo(X_train, X_test, y_train, y_test)\n","  if res in occurence:\n","    occurence[res] += 1\n","  else:\n","    occurence[res] = 1\n","\n","print(occurence)"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyMSWdTuRRS14fSPDK3WLTw4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
