{"cells":[{"cell_type":"markdown","metadata":{"id":"nkcMVRfc-eW0"},"source":["**6. (25%, coding assignment) Could you implement your own KNN function from scratch (without calling any existing classifier packages)? Your own KNN function must have following parameters to tune: n_neighbors, weights. Can you compare your own classifier with sklearn neighbors.KNeighborsClassifier in terms performance, complexity, etc. Data usage: datasets.load_iris() from sklearn.**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UGtI1nQeYea6"},"outputs":[],"source":["import math\n","import numpy as np\n","import pandas as pd\n","\n","class CustomKNN:\n","\n","  def __init__(self, n_neighbors=5, weight='uniform', power=2, distance='minkowski'):\n","    \n","    if weight not in ['uniform', 'distance']:\n","      raise IOError('Unrecognized weight type')\n","    if not isinstance(power, int):\n","      raise IOError('power should be an integer')\n","    if distance not in ['minkowski', 'euclidean', 'manhattan', 'cosine']:\n","      raise IOError('Distance is not recognized')\n","\n","    self._n_neighbors = n_neighbors\n","    self._weight = weight\n","    self._distance = distance\n","    if distance == 'manhattan':\n","      self._power = 1\n","    elif distance == 'euclidean':\n","      self._power = 2\n","    else:\n","      self._power = power\n","\n","  # Showing the parameters used in this classifier\n","  def get_params(self):\n","    params = {\n","        'n_neighbors': self._n_neighbors,\n","        'weight': self._weight,\n","        'power': self._power,\n","        'distance': self._distance\n","    }\n","    return params\n","\n","  def euclidean_distance(self, query, data):\n","    ed = sum([(query[i] - data[i]) ** 2 for i in range(len(query))])\n","    ed = math.sqrt(ed)\n","    return ed\n","\n","  def manhattan_distance(self, query, data):\n","    md = sum([math.abs(query[i] - data[i]) for i in range(len(query))])\n","    return md\n","\n","  def minkowski_distance(self, query, data):\n","    if self._power == 1:\n","      return self.manhattan_distance(query, data)\n","    elif self._power == 2:\n","      return self.euclidean_distance(query, data)\n","    md = [math.pow(query[i] - data[i], self._power) for i in range(len(query))]\n","    md = sum(md)\n","    md = math.pow(md, 1 / self._power)\n","    return md\n","\n","  def cosine_similarity(self, query, data):\n","    cs = sum([query[i] * data[i] for i in range(len(query))])\n","    deno_q = math.sqrt(sum([q ** 2 for q in query]))\n","    deno_d = math.sqrt(sum([d ** 2 for d in data]))\n","    cs = cs / (deno_q * deno_d)\n","    return cs\n","\n","  def find_neighbors(self, query, dataset):\n","    distances = []\n","    distance_function = self.cosine_similarity if self._distance == 'cosine' else self.minkowski_distance\n","    for data in dataset:\n","      dist = distance_function(query, data)\n","      distances.append((data, dist))\n","    distances.sort(key=lambda t: t[1])\n","\n","    neighbors = []\n","    for i in range(self._n_neighbors):\n","      neighbors.append(distances[i][0])\n","    return neighbors\n","\n","  # Implemented to find the indices from the descriptive features \n","  # to find the targets\n","  def compare_sublist(self, des_fea, nei_fea):\n","    nei_idx = []\n","    for des in des_fea:\n","      # all neighbors are removed from the list, stop searching\n","      if nei_fea is None:\n","        break\n","\n","      rmb = None\n","      for nei in nei_fea:\n","        if all([des[i] == nei[i] for i in range(len(nei))]):\n","          # Remember the neighbor index that is found\n","          rmb = nei_fea.index(nei)\n","          nei_idx.append(des_fea.index(des))\n","          break\n","      # Pop the searched neighbor to reduce iterations\n","      if rmb:\n","        nei_fea.pop(rmb)\n","\n","    return nei_idx\n","\n","  def flatten(self, lst):\n","    res = []\n","    for it in lst:\n","        if isinstance(it, list):\n","            res.extend(self.flatten(it))\n","        else:\n","            res.append(it)\n","    return res\n","\n","  # Simple check to make sure all values in the descriptive features are numerical\n","  def validate_inputs(self, des_fea, queries):\n","    flattened = self.flatten(des_fea)\n","    return not any([isinstance(v, str) for v in flattened])\n","\n","  # Improved ChatGPT suggestion\n","  # Prompt: can you suggest a way to convert any iterable that is not a list type into a list in Python?\n","  def to_list(self, it):\n","    if isinstance(it, list):\n","        return it\n","    elif isinstance(it, tuple) or isinstance(it, set):\n","        return list(it)\n","    elif isinstance(it, dict):\n","        return list(it.values())\n","    elif isinstance(it, pd.DataFrame):\n","        return it.values.tolist()\n","    elif isinstance(it, np.ndarray):\n","        return it.tolist()\n","    else:\n","        raise TypeError(\"Object is not iterable\")\n","\n","  def predict(self, queries, descriptives, targets):\n","    queries = self.to_list(queries)\n","    descriptives = self.to_list(descriptives)\n","    if not self.validate_inputs(descriptives, queries):\n","      raise IOError('Descriptive features must be numerical')\n","\n","    targets = self.to_list(targets)\n","    target_type = 'c' if len(np.unique(targets)) <= 5 else 'r'  # check what kind of prediction to make\n","\n","    predictions = []\n","    for query in queries:\n","      neighbors = self.find_neighbors(query, descriptives)\n","      neighbors_idx = self.compare_sublist(descriptives, neighbors)\n","\n","      outputs = [targets[idx] for idx in neighbors_idx]\n","      if target_type == 'c':\n","        predictions.append(max(set(outputs), key=outputs.count))\n","      else:\n","        predictions.append(sum(outputs) / len(outputs))\n","    return predictions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O8YqNRZWN0mk"},"outputs":[],"source":["from sklearn import neighbors, datasets\n","\n","iris = datasets.load_iris()\n","\n","X = iris.data[:, :2]\n","y = iris.target\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=123)\n","\n","# print(X_train)"]},{"cell_type":"markdown","metadata":{"id":"86deAJiDBcEp"},"source":["The following sections will be comparing the complexity and performance of the custom KNN classifier and the KNN classifier from Scikit learn"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1679015150674,"user":{"displayName":"hon lam chung","userId":"11702692533387331032"},"user_tz":420},"id":"nLiisNJYN7f4","outputId":"706e55fb-1dae-4e1e-a7c7-c0b64e8a7050"},"outputs":[{"name":"stdout","output_type":"stream","text":["Parameters\n","{'n_neighbors': 5, 'weight': 'uniform', 'power': 2, 'distance': 'minkowski'}\n","Total execution time: 0.032500267028808594\n","Accuracy score: 0.7631578947368421\n"]}],"source":["import time\n","from sklearn.metrics import accuracy_score\n","cknn = CustomKNN()\n","\n","print(\"Parameters\")\n","print(cknn.get_params())\n","\n","start_time = time.time()\n","y_pred = cknn.predict(X_test, X_train, y_train)\n","end_time = time.time()\n","print(\"Total execution time: {}\".format(end_time - start_time))\n","print(\"Accuracy score: {}\".format(accuracy_score(y_pred, y_test)))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":421,"status":"ok","timestamp":1679015363064,"user":{"displayName":"hon lam chung","userId":"11702692533387331032"},"user_tz":420},"id":"LSydo4d7h-nr","outputId":"3c86d872-b2e1-4017-c61f-d6f73573a575"},"outputs":[{"name":"stdout","output_type":"stream","text":["Parameters\n","{'algorithm': 'auto', 'leaf_size': 30, 'metric': 'minkowski', 'metric_params': None, 'n_jobs': None, 'n_neighbors': 5, 'p': 2, 'weights': 'uniform'}\n","Total execution time: 0.0033164024353027344\n","Accuracy score: 0.7631578947368421\n","\n","Brute force\n","Parameters\n","{'algorithm': 'auto', 'leaf_size': 30, 'metric': 'minkowski', 'metric_params': None, 'n_jobs': None, 'n_neighbors': 5, 'p': 2, 'weights': 'uniform'}\n","Total execution time: 0.09224963188171387\n","Accuracy score: 0.7631578947368421\n"]}],"source":["from sklearn.model_selection import train_test_split\n","from sklearn.neighbors import KNeighborsClassifier\n","\n","knn = KNeighborsClassifier()\n","\n","print(\"Parameters\")\n","print(knn.get_params())\n","\n","knn.fit(X_train, y_train)\n","\n","start_time = time.time()\n","y_pred = knn.predict(X_test)\n","end_time = time.time()\n","print(\"Total execution time: {}\".format(end_time - start_time))\n","print(\"Accuracy score: {}\".format(accuracy_score(y_pred, y_test)))\n","\n","\n","\n","print(\"\\nBrute force\")\n","knn_br = KNeighborsClassifier(algorithm='brute')\n","\n","print(\"Parameters\")\n","print(knn.get_params())\n","\n","knn_br.fit(X_train, y_train)\n","\n","start_time = time.time()\n","y_pred = knn_br.predict(X_test)\n","end_time = time.time()\n","print(\"Total execution time: {}\".format(end_time - start_time))\n","print(\"Accuracy score: {}\".format(accuracy_score(y_pred, y_test)))\n"]},{"cell_type":"markdown","metadata":{"id":"uBA6wddBF5kL"},"source":["For the accuracy performance, under the same parameters, the custom KNN classifer performs as well as the scikit learn KNN classifier. Since the custom KNN modifier uses a brute force approach to find the neighbors, the accuracy of the custom classifier should, in most cases, perform the same as the scikit learn model.\n","\n","For the custom KNN classifier, it is using brute force to find the neighbors. The first scikit learn KNN classifier uses a automatic algorithm selector to find neighbors. The execution time on the prediction is 10 times faster than the custom KNN classifers. However, for the second KNN classifier who I specified the algorithm to be 'brute', the execution time is almost 3 times slower than the custom KNN classifier. The complexity for the custom KNN classifier is O(n * k * d * q), where n is the number of observations, d is the number of features, q is the number of queries and k is the number of neighbors. This time is mostly spent on finding the neighbors per query, and when locating the indicies for finding the targets. According to the documentation, the complexity of scikit learn KNN classifier is O(n * d). This is the complexity spent on training. In comparison, the scikit learn KNN model will outpeform the custom KNN model in most cases. I think the algorithm it chooses is a big factor of the time complexity.\n","\n","For the memory complexity, the custom KNN classifier does not store any datasets. Therefore, the memory complexity is O(1). For the scikit learn KNN classifier, since it stores the dataset into the classifier, the complexity is O(n * d).\n","\n"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyPNiZdEeli6SZAcOaztoht5","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
